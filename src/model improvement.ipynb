{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_PATH = Path() / \"../data\"\n",
    "DATA_PATH.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "def load_data(filename, data_path=DATA_PATH,encoding='ISO-8859-1'):\n",
    "    csv_path = data_path / filename\n",
    "    return pd.read_csv(csv_path,encoding=encoding)\n",
    "\n",
    "def save_data(data, filename, data_path=DATA_PATH,encoding='ISO-8859-1'):\n",
    "    csv_path = data_path / filename\n",
    "    data.to_csv(csv_path, index=False,encoding='ISO-8859-1')\n",
    "\n",
    "PLOT_PATH = Path() / \"../plot\"\n",
    "PLOT_PATH.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300, transparent=True):\n",
    "    path = PLOT_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution, transparent=transparent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(\"processed_rev1.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\TYS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TYS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def extract_extended_phrases(text):\n",
    "    # Tokenize and POS tag\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Generate bigrams and trigrams\n",
    "    bi_grams = list(ngrams(tagged_tokens, 2))\n",
    "    tri_grams = list(ngrams(tagged_tokens, 3))\n",
    "    \n",
    "    # Define patterns to look for in bigrams\n",
    "    patterns = [\n",
    "        ('JJ', 'NN'),  # Adjective-Noun\n",
    "        ('NN', 'NN'),  # Noun-Noun\n",
    "        ('RB', 'JJ'),  # Adverb-Adjective (assuming no check for negations here)\n",
    "        ('VB', 'RB'),  # Verb-Adverb\n",
    "        ('VB', 'NN')   # Verb-Noun\n",
    "    ]\n",
    "    \n",
    "    # Define patterns to look for in trigrams\n",
    "    tri_patterns = [\n",
    "        ('JJ', 'NN', 'NN'),  # Adjective-Noun-Noun\n",
    "        ('RB', 'JJ', 'NN'),  # Adverb-Adjective-Noun\n",
    "        ('NN', 'IN', 'NN')   # Noun-Preposition-Noun\n",
    "    ]\n",
    "    \n",
    "    # Extract phrases based on bigram patterns\n",
    "    phrases = [' '.join([a, b]) for ((a, tag_a), (b, tag_b)) in bi_grams if (tag_a, tag_b) in patterns]\n",
    "    \n",
    "    # Extract phrases based on trigram patterns\n",
    "    phrases += [' '.join([a, b, c]) for ((a, tag_a), (b, tag_b), (c, tag_c)) in tri_grams if (tag_a, tag_b, tag_c) in tri_patterns]\n",
    "    \n",
    "    return phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(extract_extended_phrases(row['processed_FullDescription']), row['Target']) for index, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phrases = []\n",
    "for document, _ in documents:\n",
    "    # Since document is already a list, directly extend all_phrases with it\n",
    "    all_phrases.extend(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequency distribution of these phrases\n",
    "from nltk import FreqDist\n",
    "all_phrases_freq = FreqDist(all_phrases)\n",
    "\n",
    "# Select the most common phrases as features\n",
    "word_features = [phrase for phrase, count in all_phrases_freq.most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[f'contains({word})'] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of (document, label) pairs\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77\n"
     ]
    }
   ],
   "source": [
    "# Model \n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporate other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POS_features'] = df['processed_FullDescription'].apply(extract_extended_phrases)\n",
    "\n",
    "# Join the extracted features into a single string per document, if not already\n",
    "df['POS_features'] = df['POS_features'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Category + Location: 0.722\n",
      "Accuracy of Category + Title: 0.792\n",
      "Accuracy of Category + POS: 0.76\n",
      "Accuracy of Location + Title: 0.774\n",
      "Accuracy of Location + POS: 0.764\n",
      "Accuracy of Title + POS: 0.776\n",
      "Accuracy of All Features: 0.768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Assuming 'POS_features' column exists and 'Target' is the label column\n",
    "\n",
    "# Initialize vectorizers\n",
    "category_vectorizer = CountVectorizer()\n",
    "location_vectorizer = CountVectorizer()\n",
    "title_vectorizer = CountVectorizer()\n",
    "pos_vectorizer = CountVectorizer()\n",
    "\n",
    "# Vectorize features\n",
    "category_features = category_vectorizer.fit_transform(df['Category'])\n",
    "location_features = location_vectorizer.fit_transform(df['LocationNormalized'])\n",
    "title_features = title_vectorizer.fit_transform(df['Title'])\n",
    "pos_features = pos_vectorizer.fit_transform(df['POS_features'])\n",
    "\n",
    "# Define combinations of features to test\n",
    "feature_combinations = {\n",
    "    'Category + Location': hstack([category_features, location_features]),\n",
    "    'Category + Title': hstack([category_features, title_features]),\n",
    "    'Category + POS': hstack([category_features, pos_features]),\n",
    "    'Location + Title': hstack([location_features, title_features]),\n",
    "    'Location + POS': hstack([location_features, pos_features]),\n",
    "    'Title + POS': hstack([title_features, pos_features]),\n",
    "    'All Features': hstack([category_features, location_features, title_features, pos_features])\n",
    "}\n",
    "\n",
    "# Prepare labels\n",
    "y = df['Target']\n",
    "\n",
    "# Loop through each combination\n",
    "for combo_name, combo_features in feature_combinations.items():\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combo_features, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize and train the classifier\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    print(f\"Accuracy of {combo_name}: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemening POS tagging on 'Category', 'LocationNormalized', and 'Title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the lists of phrases into strings\n",
    "df['Category_POS'] = df['Category'].apply(lambda x: ' '.join(extract_extended_phrases(x)))\n",
    "df['Location_POS'] = df['LocationNormalized'].apply(lambda x: ' '.join(extract_extended_phrases(x)))\n",
    "df['Title_POS'] = df['Title'].apply(lambda x: ' '.join(extract_extended_phrases(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with POS_features, Location_POS: 0.768\n",
      "Accuracy with POS_features, Title_POS: 0.766\n",
      "Accuracy with Location_POS, POS_features: 0.768\n",
      "Accuracy with Category_POS, Location_POS, Title_POS, POS_features: 0.774\n",
      "Accuracy with Category_POS, POS_features: 0.762\n"
     ]
    }
   ],
   "source": [
    "# Vectorize features after ensuring they are in string format\n",
    "vectorizers = {\n",
    "    'Category_POS': CountVectorizer(),\n",
    "    'Location_POS': CountVectorizer(),\n",
    "    'Title_POS': CountVectorizer(),\n",
    "    'POS_features': CountVectorizer()\n",
    "    # Add any other fields you want to vectorize\n",
    "}\n",
    "\n",
    "# Vectorize features\n",
    "vectorized_features = {name: vect.fit_transform(df[name]) for name, vect in vectorizers.items()}\n",
    "\n",
    "# Define combinations of features to test\n",
    "feature_combinations = [\n",
    "    ['POS_features', 'Location_POS'],\n",
    "    ['POS_features', 'Title_POS'],\n",
    "    ['Location_POS', 'POS_features'],\n",
    "    ['Category_POS', 'Location_POS', 'Title_POS','POS_features'],\n",
    "    ['Category_POS', 'POS_features'],\n",
    "]\n",
    "\n",
    "# Prepare labels\n",
    "y = df['Target']\n",
    "\n",
    "# Loop through each combination, train, and evaluate the classifier\n",
    "for combo in feature_combinations:\n",
    "    combined_features = hstack([vectorized_features[feature] for feature in combo])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined_features, y, test_size=0.2, random_state=42)\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    accuracy = classifier.score(X_test, y_test)\n",
    "    print(f\"Accuracy with {', '.join(combo)}: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
